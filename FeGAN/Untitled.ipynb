{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a6b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "tf.debugging.experimental.disable_dump_debug_info\n",
    "#-----------------------------------------------------------------------------------------#\n",
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from tensorflow import keras\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import argparse\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"FeGAN OD\")\n",
    "    parser.add_argument(\"--path\", default=\"../Resources/Datasets/Arrhythmia_withoutdupl_norm_02_v01.arff\",\n",
    "                        help=\"Data path\")\n",
    "    parser.add_argument(\"--lr_gen\", type=float, default=0.01, help=\"Learning rate generator\")\n",
    "    parser.add_argument(\"--lr_dis\", type=float, default=0.01, help=\"Learning rate discriminator\")\n",
    "    parser.add_argument(\"--stop_epochs\", type=int, default = 5, help=\"Generator stops training after stop_epochs\")\n",
    "    parser.add_argument(\"--k\", type=int, default=10, help=\"Number of discriminators\")\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "'''\n",
    "    Calculate AUC and print it\n",
    "'''\n",
    "def AUC(truth, decision):\n",
    "    output = metrics.roc_auc_score(truth, decision)\n",
    "    print(\"AUC: \" + str(output))\n",
    "    return output\n",
    "\n",
    "def set_seed(seed):\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    keras.utils.set_random_seed(seed) #seeds numpy, random and tf all at once\n",
    "    os.environ[\"PYTHONHASSEED\"] = str(seed)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "'''\n",
    "    Create the generator. Uses two dense layers and relu activatoin\n",
    "'''\n",
    "def create_gen():\n",
    "    generator = Sequential()\n",
    "    generator.add(layers.Dense(latent_size, input_dim=latent_size, activation=\"relu\", kernel_initializer=keras.initializers.Identity(gain=1.0)))\n",
    "    generator.add(layers.Dense(latent_size, activation='relu', kernel_initializer=keras.initializers.Identity(gain=1.0)))\n",
    "    latent = keras.Input(shape=(latent_size,))\n",
    "    fake_data = generator(latent)\n",
    "    return keras.Model(latent, fake_data)\n",
    "\n",
    "'''\n",
    "    Create the discriminator. USes two dense layers and relu activation\n",
    "'''\n",
    "def create_dis(sub_size):\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(layers.Dense(np.ceil(np.sqrt(data_size)), input_dim=sub_size, activation='relu', kernel_initializer= keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None)))\n",
    "    discriminator.add(layers.Dense(1, activation='sigmoid', kernel_initializer=keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None)))\n",
    "    data = keras.Input(shape=(sub_size,))\n",
    "    fake = discriminator(data)\n",
    "    return keras.Model(data, fake)\n",
    "\n",
    "def load_data():\n",
    "    arff_data = arff.loadarff(\"../Resources/Datasets/Arrhythmia_withoutdupl_norm_02_v01.arff\")\n",
    "    df = pd.DataFrame(arff_data[0])\n",
    "    df[\"outlier\"] = pd.factorize(df[\"outlier\"], sort=True)[0] #maybe flip\n",
    "    data_x = df.iloc[:,:-2]\n",
    "    data_y = df.iloc[:,-1]\n",
    "    \n",
    "    return data_x, data_y\n",
    "\n",
    "'''\n",
    "    Plot the loss of the models. Generator in blue.\n",
    "'''\n",
    "def plot(train_history):\n",
    "    dy = train_history['discriminator_loss']\n",
    "    gy = train_history['generator_loss']\n",
    "    auc_y = train_history['auc']\n",
    "    for i in range(k):\n",
    "        names['dy_' + str(i)] = train_history['sub_discriminator{}_loss'.format(i)]\n",
    "    x = np.linspace(1, len(gy), len(gy))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, gy, color='blue')\n",
    "    ax.plot(x, dy,color='red')\n",
    "    ax.plot(x, auc_y, color='yellow', linewidth = '3')\n",
    "    for i in range(k):\n",
    "        ax.plot(x, names['dy_' + str(i)], color='green', linewidth='0.5')\n",
    "    plt.show()\n",
    "    \n",
    "'''\n",
    "    Randomly draw subspaces for each sub_discriminator. Store them in names[]\n",
    "'''\n",
    "def draw_subspaces(dimension, k):\n",
    "    dims = random.sample(range(dimension), k)\n",
    "    for i in range(k):\n",
    "        names[\"subspaces\"+str(i)] = random.sample(range(dimension), dims[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d0f168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(777)\n",
    "train = True\n",
    "    \n",
    "#args = parse_arguments()\n",
    "data_x, data_y = load_data()\n",
    "data_size = data_x.shape[0] # n := number of samples\n",
    "latent_size = data_x.shape[1] # dimension of the data set\n",
    "\n",
    "k = 10\n",
    "train_history = defaultdict(list)\n",
    "names = locals()\n",
    "epochs = 5 * 3\n",
    "stop = 0\n",
    "\n",
    "        \n",
    "generator = create_gen()\n",
    "generator.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), loss='binary_crossentropy')\n",
    "latent = keras.Input(shape=(latent_size,))\n",
    "create = 0 # used to initialize sum of the sub_discriminators\n",
    "        \n",
    "draw_subspaces(latent_size,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23b253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 259), dtype=tf.float32, name=None), name='model/sequential/dense_1/Relu:0', description=\"created by layer 'model'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 259), dtype=tf.float32, name=None), name='model/sequential/dense_1/Relu:0', description=\"created by layer 'model'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 259), dtype=tf.float32, name=None), name='model/sequential/dense_1/Relu:0', description=\"created by layer 'model'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 259), dtype=tf.float32, name=None), name='model/sequential/dense_1/Relu:0', description=\"created by layer 'model'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 259), dtype=tf.float32, name=None), name='model/sequential/dense_1/Relu:0', description=\"created by layer 'model'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 259), dtype=tf.float32, name=None), name='model/sequential/dense_1/Relu:0', description=\"created by layer 'model'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 259), dtype=tf.float32, name=None), name='model/sequential/dense_1/Relu:0', description=\"created by layer 'model'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 259), dtype=tf.float32, name=None), name='model/sequential/dense_1/Relu:0', description=\"created by layer 'model'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 259), dtype=tf.float32, name=None), name='model/sequential/dense_1/Relu:0', description=\"created by layer 'model'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 259), dtype=tf.float32, name=None), name='model/sequential/dense_1/Relu:0', description=\"created by layer 'model'\")\n"
     ]
    }
   ],
   "source": [
    "names[\"sub_discriminator_sum\"] = 0\n",
    "for i in range(k):\n",
    "    names[\"sub_discriminator\" + str(i)] = create_dis(len(names[\"subspaces\"+str(i)]))\n",
    "    names[\"fake\" + str(i)] = generator(latent) # generate the fake data of the generator\n",
    "    #names[\"sub_discriminator\" + str(i)].trainable = False\n",
    "    print(names[\"fake\"+str(i)])\n",
    "    names[\"fake\" + str(i)] = names[\"sub_discriminator\" + str(i)](tf.gather(names[\"fake\"+str(i)],names[\"subspaces\"+str(i)],axis=1))\n",
    "    #names[\"fake\" + str(i)] = names[\"sub_discriminator\" + str(i)](tf.gather(names[\"fake\" + str(i)],names[\"subspaces\"+str(i)], axis=1)) # D(G(z))\n",
    "    #names[\"sub_discriminator_sum\"] = tf.zeros((latent_size,)) # das ist evtl falsch\n",
    "\n",
    "    names[\"sub_discriminator_sum\"] += names[\"fake\" + str(i)]\n",
    "    names[\"sub_discriminator\" + str(i)].compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), loss='binary_crossentropy')\n",
    "\n",
    "names[\"sub_discriminator_sum\"] /= k\n",
    "names[\"combine_model\"] = keras.Model(latent, names[\"sub_discriminator_sum\"]) # model with the average decision. Used to train the generator.\n",
    "names[\"combine_model\"].compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dde55e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 586us/step\n",
      "(496,)\n",
      "8/8 [==============================] - 0s 641us/step\n",
      "(496,)\n",
      "8/8 [==============================] - 0s 621us/step\n",
      "(496,)\n",
      "8/8 [==============================] - 0s 573us/step\n",
      "(496,)\n",
      "8/8 [==============================] - 0s 548us/step\n",
      "(496,)\n",
      "8/8 [==============================] - 0s 603us/step\n",
      "(496,)\n",
      "8/8 [==============================] - 0s 600us/step\n",
      "(496,)\n",
      "8/8 [==============================] - 0s 599us/step\n",
      "(496,)\n",
      "8/8 [==============================] - 0s 579us/step\n",
      "(496,)\n",
      "8/8 [==============================] - 0s 584us/step\n",
      "(496,)\n",
      "8/8 [==============================] - 0s 650us/step\n",
      "(496,)\n",
      "8/8 [==============================] - 0s 596us/step\n",
      "(496,)\n",
      "8/8 [==============================] - 0s 604us/step\n",
      "(496,)\n",
      "8/8 [==============================] - 0s 567us/step\n",
      "(496,)\n",
      "8/8 [==============================] - 0s 654us/step\n",
      "(496,)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "            #print('Epoch {} of {}'.format(epoch + 1, epochs))\n",
    "            batch_size = min(500, data_size)\n",
    "            num_batches = int(data_size / batch_size)\n",
    "        \n",
    "            for idx in range(num_batches):\n",
    "                #print('\\nTesting for epoch {} index {}:'.format(epoch + 1, idx + 1))\n",
    "\n",
    "                # Generate noise\n",
    "                noise_size = batch_size\n",
    "                noise = np.random.uniform(0, 1, (int(noise_size), latent_size))\n",
    "                \n",
    "                data_batch = data_x[idx * batch_size: (idx + 1) * batch_size]\n",
    "                \n",
    "                names[\"generated_data\"] = generator.predict(noise, verbose = 1)\n",
    "                #print(tf.shape(names[\"subspaces\"+str(i)]))\n",
    "                X = np.concatenate((data_batch, names[\"generated_data\"]))\n",
    "                Y = np.array([1] * batch_size + [0] * int(noise_size)) # 1 real, fake\n",
    "                \n",
    "                print(np.shape(Y))\n",
    "                \n",
    "                \n",
    "\n",
    "                if epoch +1 > 15:\n",
    "                        stop = 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42900d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
