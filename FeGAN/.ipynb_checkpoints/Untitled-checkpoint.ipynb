{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28a6b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "tf.debugging.experimental.disable_dump_debug_info\n",
    "#-----------------------------------------------------------------------------------------#\n",
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from tensorflow import keras\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import argparse\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"FeGAN OD\")\n",
    "    parser.add_argument(\"--path\", default=\"../Resources/Datasets/Arrhythmia_withoutdupl_norm_02_v01.arff\",\n",
    "                        help=\"Data path\")\n",
    "    parser.add_argument(\"--lr_gen\", type=float, default=0.01, help=\"Learning rate generator\")\n",
    "    parser.add_argument(\"--lr_dis\", type=float, default=0.01, help=\"Learning rate discriminator\")\n",
    "    parser.add_argument(\"--stop_epochs\", type=int, default = 5, help=\"Generator stops training after stop_epochs\")\n",
    "    parser.add_argument(\"--k\", type=int, default=10, help=\"Number of discriminators\")\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "'''\n",
    "    Calculate AUC and print it\n",
    "'''\n",
    "def AUC(truth, decision):\n",
    "    output = metrics.roc_auc_score(truth, decision)\n",
    "    print(\"AUC: \" + str(output))\n",
    "    return output\n",
    "\n",
    "def set_seed(seed):\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    keras.utils.set_random_seed(seed) #seeds numpy, random and tf all at once\n",
    "    os.environ[\"PYTHONHASSEED\"] = str(seed)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "'''\n",
    "    Create the generator. Uses two dense layers and relu activatoin\n",
    "'''\n",
    "def create_gen():\n",
    "    generator = Sequential()\n",
    "    generator.add(layers.Dense(latent_size, input_dim=latent_size, activation=\"relu\", kernel_initializer=keras.initializers.Identity(gain=1.0)))\n",
    "    generator.add(layers.Dense(latent_size, activation='relu', kernel_initializer=keras.initializers.Identity(gain=1.0)))\n",
    "    latent = keras.Input(shape=(latent_size,))\n",
    "    fake_data = generator(latent)\n",
    "    return keras.Model(latent, fake_data)\n",
    "\n",
    "'''\n",
    "    Create the discriminator. USes two dense layers and relu activation\n",
    "'''\n",
    "def create_dis(sub_size):\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(layers.Dense(np.ceil(np.sqrt(data_size)), input_dim=sub_size, activation='relu', kernel_initializer= keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None)))\n",
    "    discriminator.add(layers.Dense(1, activation='sigmoid', kernel_initializer=keras.initializers.VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None)))\n",
    "    data = keras.Input(shape=(sub_size,))\n",
    "    fake = discriminator(data)\n",
    "    return keras.Model(data, fake)\n",
    "\n",
    "def load_data():\n",
    "    arff_data = arff.loadarff(r\"C:\\Users\\denis\\Documents\\BA\\FSS_GAN\\Resources\\Datasets\\Arrhythmia_withoutdupl_norm_02_v01.arff\")\n",
    "    df = pd.DataFrame(arff_data[0])\n",
    "    df[\"outlier\"] = pd.factorize(df[\"outlier\"], sort=True)[0] #maybe flip\n",
    "    data_x = df.iloc[:,:-2]\n",
    "    data_y = df.iloc[:,-1]\n",
    "    \n",
    "    return data_x, data_y\n",
    "\n",
    "'''\n",
    "    Plot the loss of the models. Generator in blue.\n",
    "'''\n",
    "def plot(train_history):\n",
    "    dy = train_history['discriminator_loss']\n",
    "    gy = train_history['generator_loss']\n",
    "    auc_y = train_history['auc']\n",
    "    for i in range(k):\n",
    "        names['dy_' + str(i)] = train_history['sub_discriminator{}_loss'.format(i)]\n",
    "    x = np.linspace(1, len(gy), len(gy))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, gy, color='blue')\n",
    "    ax.plot(x, dy,color='red')\n",
    "    ax.plot(x, auc_y, color='yellow', linewidth = '3')\n",
    "    for i in range(k):\n",
    "        ax.plot(x, names['dy_' + str(i)], color='green', linewidth='0.5')\n",
    "    plt.show()\n",
    "    \n",
    "'''\n",
    "    Randomly draw subspaces for each sub_discriminator. Store them in names[]\n",
    "'''\n",
    "def draw_subspaces(dimension, k):\n",
    "    dims = random.sample(range(dimension), k)\n",
    "    for i in range(k):\n",
    "        names[\"subspaces\"+str(i)] = random.sample(range(dimension), dims[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d0f168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(777)\n",
    "train = True\n",
    "    \n",
    "#args = parse_arguments()\n",
    "data_x, data_y = load_data()\n",
    "data_size = data_x.shape[0] # n := number of samples\n",
    "latent_size = data_x.shape[1] # dimension of the data set\n",
    "\n",
    "k = 10\n",
    "train_history = defaultdict(list)\n",
    "names = locals()\n",
    "epochs = 5 * 3\n",
    "stop = 0\n",
    "\n",
    "        \n",
    "generator = create_gen()\n",
    "generator.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), loss='binary_crossentropy')\n",
    "latent = keras.Input(shape=(latent_size,))\n",
    "create = 0 # used to initialize sum of the sub_discriminators\n",
    "        \n",
    "draw_subspaces(latent_size,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f23b253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 259), dtype=tf.float32, name=None), name='model_1/sequential_1/dense_3/Relu:0', description=\"created by layer 'model_1'\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"tf.__operators__.getitem_1\" (type SlicingOpLambda).\n\nOnly integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got [39, 170, 53, 0, 214, 164, 147, 86, 73, 149, 68, 27, 161, 257, 222, 256, 79, 128, 32, 208, 207, 210, 106, 150, 205, 8, 89, 156, 37, 252, 134, 19, 139, 226, 169, 70, 244, 198, 176, 127, 24, 140, 115, 43, 2, 112, 65, 55, 168, 144, 98, 22, 96, 45, 122, 190, 16, 137, 100, 143, 81, 78, 48, 218, 44, 31, 141, 249, 233, 1, 120, 235, 67, 165, 248, 254, 215, 178, 30, 253, 219, 239, 193, 167, 224, 181, 103, 108, 102, 113, 87, 187, 163, 160, 217, 91, 138, 154, 129, 13, 231, 159, 95, 250, 148, 130, 111, 38, 155, 125, 101, 3, 71, 191, 166, 132, 204]\n\nCall arguments received by layer \"tf.__operators__.getitem_1\" (type SlicingOpLambda):\n  • tensor=tf.Tensor(shape=(None, 259), dtype=float32)\n  • slice_spec=({'start': 'None', 'stop': 'None', 'step': 'None'}, ['39', '170', '53', '0', '214', '164', '147', '86', '73', '149', '68', '27', '161', '257', '222', '256', '79', '128', '32', '208', '207', '210', '106', '150', '205', '8', '89', '156', '37', '252', '134', '19', '139', '226', '169', '70', '244', '198', '176', '127', '24', '140', '115', '43', '2', '112', '65', '55', '168', '144', '98', '22', '96', '45', '122', '190', '16', '137', '100', '143', '81', '78', '48', '218', '44', '31', '141', '249', '233', '1', '120', '235', '67', '165', '248', '254', '215', '178', '30', '253', '219', '239', '193', '167', '224', '181', '103', '108', '102', '113', '87', '187', '163', '160', '217', '91', '138', '154', '129', '13', '231', '159', '95', '250', '148', '130', '111', '38', '155', '125', '101', '3', '71', '191', '166', '132', '204'])\n  • var=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14276/1938537378.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#print(names[\"subspaces\"+str(i)])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#print(tf.gather(names[\"fake\" + str(i)],[names[\"subspaces\"+str(i)]])) # Problem: choosing relevant subspaces doesnt work like that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"fake\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sub_discriminator\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"fake\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"subspaces\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# D(G(z))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sub_discriminator_sum\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\denis\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\denis\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\layers\\core\\tf_op_layer.py\u001b[0m in \u001b[0;36mhandle\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         ):\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mSlicingOpLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\denis\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling layer \"tf.__operators__.getitem_1\" (type SlicingOpLambda).\n\nOnly integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got [39, 170, 53, 0, 214, 164, 147, 86, 73, 149, 68, 27, 161, 257, 222, 256, 79, 128, 32, 208, 207, 210, 106, 150, 205, 8, 89, 156, 37, 252, 134, 19, 139, 226, 169, 70, 244, 198, 176, 127, 24, 140, 115, 43, 2, 112, 65, 55, 168, 144, 98, 22, 96, 45, 122, 190, 16, 137, 100, 143, 81, 78, 48, 218, 44, 31, 141, 249, 233, 1, 120, 235, 67, 165, 248, 254, 215, 178, 30, 253, 219, 239, 193, 167, 224, 181, 103, 108, 102, 113, 87, 187, 163, 160, 217, 91, 138, 154, 129, 13, 231, 159, 95, 250, 148, 130, 111, 38, 155, 125, 101, 3, 71, 191, 166, 132, 204]\n\nCall arguments received by layer \"tf.__operators__.getitem_1\" (type SlicingOpLambda):\n  • tensor=tf.Tensor(shape=(None, 259), dtype=float32)\n  • slice_spec=({'start': 'None', 'stop': 'None', 'step': 'None'}, ['39', '170', '53', '0', '214', '164', '147', '86', '73', '149', '68', '27', '161', '257', '222', '256', '79', '128', '32', '208', '207', '210', '106', '150', '205', '8', '89', '156', '37', '252', '134', '19', '139', '226', '169', '70', '244', '198', '176', '127', '24', '140', '115', '43', '2', '112', '65', '55', '168', '144', '98', '22', '96', '45', '122', '190', '16', '137', '100', '143', '81', '78', '48', '218', '44', '31', '141', '249', '233', '1', '120', '235', '67', '165', '248', '254', '215', '178', '30', '253', '219', '239', '193', '167', '224', '181', '103', '108', '102', '113', '87', '187', '163', '160', '217', '91', '138', '154', '129', '13', '231', '159', '95', '250', '148', '130', '111', '38', '155', '125', '101', '3', '71', '191', '166', '132', '204'])\n  • var=None"
     ]
    }
   ],
   "source": [
    "for i in range(k):\n",
    "    names[\"sub_discriminator\" + str(i)] = create_dis(len(names[\"subspaces\"+str(i)]))\n",
    "    names[\"fake\" + str(i)] = generator(latent) # generate the fake data of the generator\n",
    "            #names[\"sub_discriminator\" + str(i)].trainable = False\n",
    "\n",
    "    print(names[\"fake\" + str(i)])\n",
    "    #print(names[\"subspaces\"+str(i)])\n",
    "    #print(tf.gather(names[\"fake\" + str(i)],[names[\"subspaces\"+str(i)]])) # Problem: choosing relevant subspaces doesnt work like that\n",
    "    names[\"fake\" + str(i)] = names[\"sub_discriminator\" + str(i)](names[\"fake\" + str(i)][:,names[\"subspaces\"+str(i)]]) # D(G(z))\n",
    "    names[\"sub_discriminator_sum\"] = np.zeros(latent_size) # das ist evtl falsch\n",
    "    for i in range(k):\n",
    "        for j in range(len(names[\"subspaces\"+str(i)])):\n",
    "            names[\"sub_discriminator_sum\"][names[\"subspaces\"+str(i)][j]] += names[\"fake\"+str(i)][j]\n",
    "            \n",
    "    names[\"sub_discriminator\" + str(i)].compile(optimizer=keras.optimizers.SGD(learning_rate=args.lr_dis), loss='binary_crossentropy')\n",
    "            \n",
    "names[\"sub_discriminator_sum\"] /= k\n",
    "names[\"combine_model\"] = keras.Model(latent, names[\"sub_discriminator_sum\"]) # model with the average decision. Used to train the generator.\n",
    "names[\"combine_model\"].compile(optimizer=keras.optimizers.SGD(learning_rate=args.lr_gen), loss='binary_crossentropy')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde55e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
